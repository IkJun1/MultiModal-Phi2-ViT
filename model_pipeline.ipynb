{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f27269",
   "metadata": {},
   "source": [
    "## 1. Design Model Archetecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7a3e7",
   "metadata": {},
   "source": [
    "### 1-1. LLM, ViT load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "def load_models_cuda():\n",
    "    \"\"\"모델을 CUDA 디바이스로 로드하는 함수\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        \"microsoft/phi-2\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        load_in_4bit=True,     \n",
    "    ).to(device)\n",
    "\n",
    "    vision_encoder = CLIPVisionModel.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    ).to(device)\n",
    "\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\"\n",
    "    )\n",
    "\n",
    "    return llm, tokenizer, vision_encoder, image_processor\n",
    "                                    \n",
    "llm, tokenizer, vision_encoder, image_processor = load_models_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3233fc",
   "metadata": {},
   "source": [
    "### 1-2 Cross-Attention arhchitecture 4개 레이어에만 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, model_dims: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=model_dims, \n",
    "            num_heads=num_heads, \n",
    "            batch_first=True,\n",
    "            dtype=torch.bfloat16 # float16\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(model_dims)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        # 교차-어텐션 수행\n",
    "        attn_output, _ = self.attention(text_features, image_features, image_features)\n",
    "        # Residual Connection (잔차 연결) 및 Layer Normalization\n",
    "        # 입력(text_features)과 출력(attn_output)을 더해줌으로써 학습 안정성을 높임\n",
    "        output = self.layer_norm(text_features + attn_output)\n",
    "        return output\n",
    "\n",
    "class MultimodalPhi2(nn.Module):\n",
    "    def __init__(self, peft_llm, vision_encoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.llm = peft_llm\n",
    "\n",
    "        target_dtype = self.llm.dtype\n",
    "        model_dims = self.llm.config.hidden_size\n",
    "        vit_dims = self.vision_encoder.config.hidden_size\n",
    "        num_heads = self.llm.config.num_attention_heads\n",
    "        num_llm_layers = self.llm.config.num_hidden_layers\n",
    "\n",
    "        self.target_layers = range(num_llm_layers - 4, num_llm_layers)\n",
    "\n",
    "        self.vision_projection = nn.Linear(vit_dims, model_dims)\n",
    "        self.vision_projection.to(device=self.llm.device, dtype=target_dtype) \n",
    "        \n",
    "        ## 4개의 레이어에 대해서만 교차-어텐션을 적용\n",
    "        self.cross_attentions = nn.ModuleDict({\n",
    "        str(i): CrossAttention(model_dims, num_heads) for i in self.target_layers\n",
    "        }) \n",
    "\n",
    "        ## cross attentioin foward hook 부분\n",
    "        self.image_features_cache = None # 이미지 특징을 임시 저장할 공간\n",
    "\n",
    "        for layer_idx in self.target_layers:\n",
    "        # ModuleDict의 키는 문자열이므로, 인덱싱할 때 str(layer_idx)를 사용합니다.\n",
    "            layer = self.llm.model.model.layers[layer_idx] # peft_llm의 경우, model을 한번 더 거쳐야 함\n",
    "            layer.self_attn.register_forward_hook(\n",
    "                partial(self.cross_attention_hook, layer_idx=layer_idx)\n",
    "            )\n",
    "        \n",
    "        self.cross_attentions.to(device=self.llm.device, dtype=target_dtype)\n",
    "\n",
    "    def cross_attention_hook(self, module, input, output, layer_idx):\n",
    "        # output은 self_attn 모듈의 최종 출력값입니다.\n",
    "        # 원래 출력 형식: (hidden_states, attention_weights, past_key_value)\n",
    "        hidden_states = output[0]\n",
    "        \n",
    "        # ModuleDict의 키는 문자열이므로, 인덱싱할 때 str(layer_idx)를 사용\n",
    "        cross_attn_output = self.cross_attentions[str(layer_idx)](\n",
    "            hidden_states, self.image_features_cache\n",
    "        )\n",
    "        # 원래 출력의 형태를 유지하면서, 작업이 완료된 hidden_states로 교체하여 반환합니다.\n",
    "        return (cross_attn_output,) + output[1:]\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, pixel_values: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        # 1. 이미지 특징을 계산하고, 훅 함수가 사용할 수 있도록 캐시에 저장합니다.\n",
    "        image_outputs = self.vision_encoder(pixel_values)\n",
    "        image_patch_features = image_outputs.last_hidden_state\n",
    "        self.image_features_cache = self.vision_projection(\n",
    "            image_patch_features.to(self.llm.dtype)\n",
    "        )\n",
    "\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels \n",
    "        )\n",
    "        \n",
    "        # 3. 사용이 끝난 임시 캐시를 비워줍니다.\n",
    "        self.image_features_cache = None\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f2cd5",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728788cf",
   "metadata": {},
   "source": [
    "### 2-1. 데이터, 전처리 도구 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"clip-benchmark/wds_mscoco_captions2017\")\n",
    "\n",
    "# load vit processor and tokenizer\n",
    "from transformers import CLIPImageProcessor, AutoTokenizer\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8477f",
   "metadata": {},
   "source": [
    "### 2-2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2593921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "# --- 1. 데이터셋 '스트리밍'을 위한 제너레이터 함수 정의 (tqdm 추가) ---\n",
    "def flatten_generator(dataset):\n",
    "    \"\"\"\n",
    "    데이터셋을 순회하며, [이미지, 단일 캡션] 쌍을 하나씩 생성(yield)하고 진행 상황을 보여줍니다.\n",
    "    \"\"\"\n",
    "    print(\"Generator started. Processing data one by one...\")\n",
    "    # tqdm으로 dataset을 감싸서 진행 상황을 표시합니다.\n",
    "    # desc는 진행률 표시줄 앞에 표시될 설명입니다.\n",
    "    for item in tqdm(dataset, desc=\"Flattening dataset\"):\n",
    "        captions = [line for line in item['txt'].split('\\n')]\n",
    "        for caption in captions:\n",
    "            yield {'jpg': item['jpg'], 'caption': caption}\n",
    "\n",
    "# --- 2. 제너레이터로부터 새로운 데이터셋 생성 (이전과 동일) ---\n",
    "print(\"Creating a new dataset from the generator (memory-efficient)...\")\n",
    "expanded_dataset = Dataset.from_generator(flatten_generator, gen_kwargs={\"dataset\": dataset['train']})\n",
    "print(f\"Dataset created with {len(expanded_dataset)} samples.\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer's pad_token has been set to eos_token.\")\n",
    "\n",
    "# --- 3. .map()을 위한 전처리 함수 정의 (이전과 동일) ---\n",
    "def preprocess_function(examples):\n",
    "    # ... (이전 답변의 preprocess_function 코드와 동일)\n",
    "    images = [img.convert(\"RGB\") for img in examples['jpg']]\n",
    "    captions = examples['caption']\n",
    "    model_inputs = image_processor(images, return_tensors=\"pt\")\n",
    "    text_inputs = tokenizer(captions, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    model_inputs['input_ids'] = text_inputs['input_ids']\n",
    "    model_inputs['attention_mask'] = text_inputs['attention_mask']\n",
    "    model_inputs['labels'] = text_inputs['input_ids'].clone()\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# --- 4. .map() 함수로 '전처리 공장' 가동 (자동으로 진행률 표시) ---\n",
    "print(\"\\nStarting memory-efficient preprocessing with .map()...\")\n",
    "\n",
    "final_dataset = expanded_dataset.map(\n",
    "    function=preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=expanded_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing and caching complete!\")\n",
    "print(\"\\n--- Final Processed Dataset Info ---\")\n",
    "print(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace809b3",
   "metadata": {},
   "source": [
    "### 2-3. 데이터 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_save_path = \"./my_final_dataset\"\n",
    "print(f\"Saving the final processed dataset to '{final_save_path}'...\")\n",
    "final_dataset.save_to_disk(final_save_path)\n",
    "print(\"Final dataset saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf94c0",
   "metadata": {},
   "source": [
    "### 2-4. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "final_save_path = \"./my_final_dataset\"\n",
    "print(f\"Loading the final preprocessed dataset from '{final_save_path}'...\")\n",
    "\n",
    "# 최종 저장된 데이터셋을 바로 불러옵니다.\n",
    "final_dataset = load_from_disk(final_save_path)\n",
    "\n",
    "print(\"Dataset loaded instantly from disk!\")\n",
    "print(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aebbfc",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a00a0",
   "metadata": {},
   "source": [
    "### 3-1. Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "\n",
    "\"\"\"모델을 CUDA 디바이스로 로드하는 함수\"\"\"\n",
    "def load_models_cuda():\n",
    "    \"\"\"모델을 CUDA 디바이스로 로드하는 함수\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        \"microsoft/phi-2\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        load_in_4bit=True,     \n",
    "    ).to(device)\n",
    "\n",
    "    vision_encoder = CLIPVisionModel.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    ).to(device)\n",
    "\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\"\n",
    "    )\n",
    "\n",
    "    return tokenizer, llm, vision_encoder, image_processor\n",
    "\n",
    "tokenizer, llm, vision_encoder, image_processor = load_models_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb81074",
   "metadata": {},
   "source": [
    "### 3-2. Connet LoRA example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16, # LoRA를 이용해 몇차원 으로 줄일지 설정\n",
    "    lora_alpha=32, # LoRA의 영향력 조절\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"dense\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "peft_llm = get_peft_model(llm, lora_config)\n",
    "peft_llm.print_trainable_parameters()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "multi_modal_model = MultimodalPhi2(peft_llm, vision_encoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303c6d8",
   "metadata": {},
   "source": [
    "### 3-3. Find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ad986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW \n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import default_data_collator\n",
    "from tqdm.auto import tqdm\n",
    "import wandb \n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import gc\n",
    "\n",
    "\n",
    "def model_train_eval(llm, vision_encoder, dataset, parameters, device):\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=parameters['LoRA_R'], # LoRA를 이용해 몇차원 으로 줄일지 설정\n",
    "        lora_alpha=parameters['LoRA_R']*2, # LoRA의 영향력 조절\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"dense\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    peft_llm = get_peft_model(llm, lora_config)\n",
    "    model = MultimodalPhi2(peft_llm, vision_encoder).to(device)\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"Making-Multimodal-Models\",\n",
    "        name=f\"{config['architecture']} with LoRA R={config['LoRA_R']} LoRA alpha={config['LoRA_R']*2} lr={config['learning_rate']}\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    test_final_dataset = dataset.select(range(5000)) #훈련 상황에 따라 바꾸면 됨\n",
    "    test_eval_final_dataset = dataset.select(range(5000, 5500)) # 테스트 데이터셋\n",
    "\n",
    "    # collate_fn = defalut_data_collator를 함으로써 데이터의 형태로 올바르게 맞춰 줌\n",
    "    train_dataloader = DataLoader(test_final_dataset, batch_size=config[\"batch_size\"], collate_fn=default_data_collator, shuffle=True)\n",
    "    eval_dataloader = DataLoader(test_eval_final_dataset, batch_size=config[\"batch_size\"], collate_fn=default_data_collator, shuffle=True)\n",
    "\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad] # require_grad가 허용된(미분가능) 부분에만 optimizer적용\n",
    "    optimizer = AdamW(trainable_params, lr=config[\"learning_rate\"])\n",
    "\n",
    "    num_training_steps = config[\"num_epochs\"] * len(train_dataloader)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0, # 몇 steo동안 천천히 증가(웜업) 할지 설정\n",
    "        num_training_steps=num_training_steps, # 몇 step에 걸쳐 천천히 감소할지 설정\n",
    "    )\n",
    "    print(\"Optimizer and Scheduler have been set up.\")\n",
    "\n",
    "    # 학습 시작\n",
    "    print(f\"\\n--- Starting Training for {config['num_epochs']} epoch(s) ---\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 훈련 루프\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        print(train_dataloader)\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            outputs = model(batch['input_ids'].to(device),\n",
    "                                        batch['pixel_values'].to(device),\n",
    "                                        batch['attention_mask'].to(device),\n",
    "                                        batch['labels'].to(device))\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                    wandb.log({\"train/loss\": loss.item()})\n",
    "            \n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        # 검증 루프 \n",
    "        model.eval()\n",
    "        eval_loss_total = 0\n",
    "        print(f\"\\n--- Validating Epoch {epoch + 1} ---\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_dataloader, desc=\"Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                eval_loss_total += loss.item()\n",
    "        \n",
    "        avg_eval_loss = eval_loss_total / len(eval_dataloader)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"eval/loss\": avg_eval_loss,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "        \n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 텐서로 바꿔주기\n",
    "    columns_to_tensorize = ['pixel_values', 'input_ids', 'attention_mask', 'labels']\n",
    "    final_dataset.set_format(type='torch', columns=columns_to_tensorize)\n",
    "\n",
    "    parameters = {\n",
    "        \"learning_rate\": [5e-5, 3e-5, 1e-5],\n",
    "        \"LoRA_R\": [8, 16, 32],\n",
    "    }\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 파라미터별 데이터 성능 테스트\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            # 모델 로드\n",
    "            llm, tokenizer, vision_encoder, image_processor = load_models_cuda()\n",
    "            \n",
    "            config = {\"num_epochs\": 1,\n",
    "                            \"batch_size\": 4,\n",
    "                            \"learning_rate\": parameters['learning_rate'][i],\n",
    "                            \"LoRA_R\": parameters['LoRA_R'][j],\n",
    "                            \"architecture\": \"Corss-Attention Multimodal Phi-2\",\n",
    "                            \"dataset\": \"lip-benchmark/wds_mscoco_captions2017\"}\n",
    "            model_train_eval(llm, vision_encoder, final_dataset, config, device)\n",
    "\n",
    "            del llm, vision_encoder, image_processor, tokenizer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    print(\"Model test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b0a89",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b4a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW \n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import default_data_collator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, CLIPVisionModel, CLIPImageProcessor\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_from_disk\n",
    "import wandb \n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import os\n",
    "\n",
    "#------------------------------------------- 모델 불러오기\n",
    "def load_models_cuda():\n",
    "    \"\"\"모델을 CUDA 디바이스로 로드하는 함수\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        \"microsoft/phi-2\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        load_in_4bit=True, \n",
    "    ).to(device)\n",
    "\n",
    "    vision_encoder = CLIPVisionModel.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    ).to(device)\n",
    "\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\"\n",
    "    )\n",
    "\n",
    "    return llm, tokenizer, vision_encoder, image_processor\n",
    "\n",
    "llm, tokenizer, vision_encoder, image_processor = load_models_cuda()\n",
    "\n",
    "#------------------------------------------- 모델 아키텍쳐 작성\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, model_dims: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=model_dims, \n",
    "            num_heads=num_heads, \n",
    "            batch_first=True,\n",
    "            dtype=torch.bfloat16 # float16\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(model_dims)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        attn_output, _ = self.attention(text_features, image_features, image_features)\n",
    "        output = self.layer_norm(text_features + attn_output)\n",
    "        return output\n",
    "\n",
    "class MultimodalPhi2(nn.Module):\n",
    "    def __init__(self, peft_llm, vision_encoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.llm = peft_llm\n",
    "\n",
    "        target_dtype = self.llm.dtype\n",
    "        model_dims = self.llm.config.hidden_size\n",
    "        vit_dims = self.vision_encoder.config.hidden_size\n",
    "        num_heads = self.llm.config.num_attention_heads\n",
    "        num_llm_layers = self.llm.config.num_hidden_layers\n",
    "\n",
    "        self.target_layers = range(num_llm_layers - 4, num_llm_layers)\n",
    "\n",
    "        self.vision_projection = nn.Linear(vit_dims, model_dims)\n",
    "        self.vision_projection.to(device=self.llm.device, dtype=target_dtype) \n",
    "        \n",
    "        self.cross_attentions = nn.ModuleDict({\n",
    "        str(i): CrossAttention(model_dims, num_heads) for i in self.target_layers\n",
    "        }) \n",
    "\n",
    "        self.image_features_cache = None # 이미지 특징을 임시 저장할 공간\n",
    "\n",
    "        for layer_idx in self.target_layers:\n",
    "        # ModuleDict의 키는 문자열이므로, 인덱싱할 때 str(layer_idx)를 사용합니다.\n",
    "            layer = self.llm.model.model.layers[layer_idx] \n",
    "            layer.self_attn.register_forward_hook(\n",
    "                partial(self.cross_attention_hook, layer_idx=layer_idx)\n",
    "            )\n",
    "        \n",
    "        self.cross_attentions.to(device=self.llm.device, dtype=target_dtype)\n",
    "\n",
    "    def cross_attention_hook(self, module, input, output, layer_idx):\n",
    "\n",
    "        hidden_states = output[0]\n",
    "        \n",
    "        # ModuleDict의 키는 문자열이므로, 인덱싱할 때 str(layer_idx)를 사용\n",
    "        cross_attn_output = self.cross_attentions[str(layer_idx)](\n",
    "            hidden_states, self.image_features_cache\n",
    "        )\n",
    "        # 원래 출력의 형태를 유지하면서, 작업이 완료된 hidden_states로 교체하여 반환합니다.\n",
    "        return (cross_attn_output,) + output[1:]\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, pixel_values: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        image_outputs = self.vision_encoder(pixel_values)\n",
    "        image_patch_features = image_outputs.last_hidden_state\n",
    "        self.image_features_cache = self.vision_projection(\n",
    "            image_patch_features.to(self.llm.dtype)\n",
    "        )\n",
    "\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels \n",
    "        )\n",
    "        \n",
    "        # 3. 사용이 끝난 임시 캐시를 비워줍니다.\n",
    "        self.image_features_cache = None\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "#------------------------------------------- 모델 로드    \n",
    "final_save_path = \"./my_final_dataset\"\n",
    "eval_save_path = \"./test_dataset\"\n",
    "print(f\"Loading the final preprocessed dataset from '{final_save_path}'...\")\n",
    "\n",
    "# 최종 저장된 데이터셋을 바로 불러옵니다.\n",
    "dataset = load_from_disk(final_save_path)\n",
    "eval_dataset = load_from_disk(eval_save_path)\n",
    "\n",
    "#------------------------------------------- config 작성\n",
    "config = {\"num_epochs\": 5,\n",
    "            \"batch_size\": 4,\n",
    "            \"learning_rate\": 5e-05,\n",
    "            \"LoRA_R\": 32,\n",
    "            \"LoRA_alpha\": 64,\n",
    "            \"architecture\": \"Corss-Attention Multimodal Phi-2\",\n",
    "            \"dataset\": \"lip-benchmark/wds_mscoco_captions2017\"}\n",
    "\n",
    "#------------------------------------------- 모델 학습 \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "columns_to_tensorize = ['pixel_values', 'input_ids', 'attention_mask', 'labels']\n",
    "dataset.set_format(type='torch', columns=columns_to_tensorize)\n",
    "eval_dataset.set_format(type='torch', columns=columns_to_tensorize)\n",
    "\n",
    "config = {\"num_epochs\": 5,\n",
    "            \"batch_size\": 8,\n",
    "            \"learning_rate\": 5e-5,\n",
    "            \"LoRA_R\": 32,\n",
    "            \"LoRA_alpha\": 64,\n",
    "            \"architecture\": \"Corss-Attention Multimodal Phi-2\",\n",
    "            \"dataset\": \"lip-benchmark/wds_mscoco_captions2017\"}\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=config['LoRA_R'], # LoRA를 이용해 몇차원 으로 줄일지 설정\n",
    "    lora_alpha=config['LoRA_alpha'], # LoRA의 영향력 조절\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"dense\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "peft_llm = get_peft_model(llm, lora_config)\n",
    "model = MultimodalPhi2(peft_llm, vision_encoder).to(device)\n",
    "\n",
    "wandb.init(\n",
    "        project=\"final-multimodal-training\",\n",
    "        name=\"multimodal_phi2_training\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "# collate_fn = defalut_data_collator를 함으로써 데이터의 형태로 올바르게 맞춰 줌\n",
    "train_dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], collate_fn=default_data_collator, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=config[\"batch_size\"], collate_fn=default_data_collator, shuffle=True)\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad] # require_grad가 허용된(미분가능) 부분에만 optimizer적용\n",
    "optimizer = AdamW(trainable_params, lr=config[\"learning_rate\"])\n",
    "\n",
    "num_training_steps = config[\"num_epochs\"] * len(train_dataloader)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0, # 몇 step동안 천천히 증가(웜업) 할지 설정\n",
    "    num_training_steps=num_training_steps, # 몇 step에 걸쳐 천천히 감소할지 설정\n",
    ")\n",
    "print(\"Optimizer and Scheduler have been set up.\")\n",
    "\n",
    "# 학습 시작\n",
    "print(f\"\\n--- Starting Training for {config['num_epochs']} epoch(s) ---\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "save_directory = \"./save_model\"\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "    print(train_dataloader)\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        outputs = model(batch['input_ids'].to(device),\n",
    "                        batch['pixel_values'].to(device),\n",
    "                        batch['attention_mask'].to(device),\n",
    "                        batch['labels'].to(device))\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            wandb.log({\"train/loss\": loss.item()})\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    # 검증 루프 \n",
    "    model.eval()\n",
    "    eval_loss_total = 0\n",
    "    print(f\"\\n--- Validating Epoch {epoch + 1} ---\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            for batch in tqdm(eval_dataloader, desc=\"Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                eval_loss_total += loss.item()\n",
    "        \n",
    "    avg_eval_loss = eval_loss_total / len(eval_dataloader)\n",
    "        \n",
    "    wandb.log({\n",
    "            \"eval/loss\": avg_eval_loss,\n",
    "            \"epoch\": epoch + 1\n",
    "    })\n",
    "\n",
    "    # 모델 저장\n",
    "    os.makedirs(save_directory+f\"/{epoch}epoch\", exist_ok=True)\n",
    "    \n",
    "    lora_save_path = os.path.join(save_directory+f\"/{epoch}epoch\", f\"llm_adapters{epoch}\")\n",
    "    model.llm.save_pretrained(lora_save_path)\n",
    "    print(f\"LoRA adapters saved to {lora_save_path}\")\n",
    "\n",
    "    vision_projection_path = os.path.join(save_directory+f\"/{epoch}epoch\", f\"vision_projection{epoch}.pt\")\n",
    "    torch.save(model.vision_projection.state_dict(), vision_projection_path)\n",
    "    print(f\"Vision projection saved to {vision_projection_path}\")\n",
    "\n",
    "    cross_attentions_path = os.path.join(save_directory+f\"/{epoch}epoch\", f\"cross_attentions{epoch}.pt\")\n",
    "    torch.save(model.cross_attentions.state_dict(), cross_attentions_path)\n",
    "    print(f\"Cross attentions saved to {cross_attentions_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96472b",
   "metadata": {},
   "source": [
    "## 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_dataset = pd.read_csv(\"./open/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추론\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.functional import F\n",
    "\n",
    "def inference(model, tokenizer, image_processor, prompt, image_path):\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                print(\"Tokenizer pad_token is set to eos_token.\")\n",
    "\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = prompt_tokens.input_ids.to(device)\n",
    "    attention_mask = prompt_tokens.attention_mask.to(device)\n",
    "    PIL_image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image_processor(images=PIL_image, return_tensors=\"pt\").pixel_values.to(device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_outputs = model.vision_encoder(image)\n",
    "        image_patch_features = image_outputs.last_hidden_state\n",
    "        model.image_features_cache = model.vision_projection(image_patch_features)\n",
    "    \n",
    "    generated_ids = model.llm.generate(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        max_new_tokens=128,  # 새로 생성할 최대 토큰 수\n",
    "        do_sample=False,     # 샘플링을 활성화\n",
    "        temperature=1,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    model.image_features_cache = None\n",
    "\n",
    "    input_token_len = input_ids.shape[1]\n",
    "    generated_text_ids = generated_ids[:, input_token_len:]\n",
    "    \n",
    "    generated_text = tokenizer.batch_decode(generated_text_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text.strip()\n",
    "\n",
    "# 기존 질문 내용\n",
    "\n",
    "index = 1\n",
    "\n",
    "image_path = \"./open/\"+test_dataset['img_path'][index]\n",
    "\n",
    "question = test_dataset['Question'][index]+\" A.\"+ test_dataset['A'][index] + \" B.\" + test_dataset['B'][index] + \" C.\" + test_dataset['C'][index] + \" D.\" + test_dataset['D'][index]\n",
    "prompt = f\"\"\"ROLE: You're an ASSISTANT, I give you an image and a question, and you answer it for me.\\n \n",
    "            RULE: Please select only one correct answer from A,B,C,D\\n\n",
    "            USER: {question}\\n\n",
    "            ASSISTANT:\"\"\"\n",
    "\n",
    "result = inference(model, tokenizer, image_processor, prompt, image_path)\n",
    "\n",
    "plt.imshow(plt.imread(image_path))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Generated text: {result}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
