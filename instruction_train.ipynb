{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0629aaae",
   "metadata": {},
   "source": [
    "## 1. 모델 아키텍쳐 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/MultiModal/.venv/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, model_dims: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=model_dims, \n",
    "            num_heads=num_heads, \n",
    "            batch_first=True,\n",
    "            dtype=torch.bfloat16 # float16\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(model_dims)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        attn_output, _ = self.attention(text_features, image_features, image_features)\n",
    "        output = self.layer_norm(text_features + attn_output)\n",
    "        return output\n",
    "\n",
    "class MultimodalPhi2(nn.Module):\n",
    "    def __init__(self, peft_llm, vision_encoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.llm = peft_llm\n",
    "\n",
    "        target_dtype = self.llm.dtype\n",
    "        model_dims = self.llm.config.hidden_size\n",
    "        vit_dims = self.vision_encoder.config.hidden_size\n",
    "        num_heads = self.llm.config.num_attention_heads\n",
    "        num_llm_layers = self.llm.config.num_hidden_layers\n",
    "\n",
    "        self.target_layers = range(num_llm_layers - 4, num_llm_layers)\n",
    "\n",
    "        self.vision_projection = nn.Linear(vit_dims, model_dims)\n",
    "        self.vision_projection.to(device=self.llm.device, dtype=target_dtype) \n",
    "        \n",
    "        self.cross_attentions = nn.ModuleDict({\n",
    "        str(i): CrossAttention(model_dims, num_heads) for i in self.target_layers\n",
    "        }) \n",
    "\n",
    "        self.image_features_cache = None # 이미지 특징을 임시 저장할 공간\n",
    "\n",
    "        for layer_idx in self.target_layers:\n",
    "            layer = self.llm.model.model.layers[layer_idx] # peft_llm의 경우, model을 한번 더 거쳐야 함\n",
    "            layer.self_attn.register_forward_hook(\n",
    "                partial(self.cross_attention_hook, layer_idx=layer_idx)\n",
    "            )\n",
    "        \n",
    "        self.cross_attentions.to(device=self.llm.device, dtype=target_dtype)\n",
    "\n",
    "    def cross_attention_hook(self, module, input, output, layer_idx):\n",
    "        hidden_states = output[0]\n",
    "        \n",
    "        # ModuleDict의 키는 문자열이므로, 인덱싱할 때 str(layer_idx)를 사용\n",
    "        cross_attn_output = self.cross_attentions[str(layer_idx)](\n",
    "            hidden_states, self.image_features_cache\n",
    "        )\n",
    "\n",
    "        return (cross_attn_output,) + output[1:]\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, pixel_values: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        # 이미지 특징을 계산하고, 훅 함수가 사용할 수 있도록 캐시에 저장\n",
    "        image_outputs = self.vision_encoder(pixel_values)\n",
    "        image_patch_features = image_outputs.last_hidden_state\n",
    "        self.image_features_cache = self.vision_projection(\n",
    "            image_patch_features.to(self.llm.dtype)\n",
    "        )\n",
    "\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels \n",
    "        )\n",
    "        \n",
    "        self.image_features_cache = None\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad85b3",
   "metadata": {},
   "source": [
    "## 2. 모델 불러오기 \n",
    "    4번 학습한 3epoch폴더가 가장 성능 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c10286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/MultiModal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, CLIPVisionModel, CLIPImageProcessor\n",
    "def load_my_trained_model(save_directory):\n",
    "\n",
    "    print(f\"Loading our trained model from {save_directory}...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", load_in_4bit=True, torch_dtype=torch.bfloat16).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "    vision_encoder = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\", torch_dtype=torch.bfloat16).to(device)\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    lora_save_path = os.path.join(save_directory, \"llm_adapters3\") # 저장 시 파일명과 일치\n",
    "    peft_llm = PeftModel.from_pretrained(llm, lora_save_path)\n",
    "\n",
    "    model = MultimodalPhi2(peft_llm, vision_encoder)\n",
    "\n",
    "\n",
    "    vision_projection_path = os.path.join(save_directory, \"vision_projection3.pt\")\n",
    "    model.vision_projection.load_state_dict(torch.load(vision_projection_path, map_location='cpu'))\n",
    "\n",
    "    cross_attentions_path = os.path.join(save_directory, \"cross_attentions3.pt\")\n",
    "    model.cross_attentions.load_state_dict(torch.load(cross_attentions_path, map_location='cpu'))\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    print(\"\\nOur final trained model has been successfully loaded and is ready for inference!\")\n",
    "    return model, tokenizer, image_processor\n",
    "\n",
    "#model, tokenizer, image_processor = load_my_trained_model(\"./save_model/3epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7827ab",
   "metadata": {},
   "source": [
    "## trainDataProcessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import CLIPImageProcessor, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "with open(\"llava_instruct_150k.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    instruct_data = json.load(f)\n",
    "\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "def instruction_generator(dataset):\n",
    "    for item in tqdm(dataset, desc=\"Generating instruction pairs\"):\n",
    "        conv = item['conversations']\n",
    "        \n",
    "        # 2개씩 짝지어 (human, gpt) 턴을 처리\n",
    "        for i in range(0, len(conv), 2):\n",
    "            human_turn = conv[i]\n",
    "            gpt_turn = conv[i+1]\n",
    "            \n",
    "            # 질문과 답변 텍스트를 분리\n",
    "            question = human_turn['value']\n",
    "            answer = gpt_turn['value']\n",
    "            \n",
    "            # 나중에 처리하기 쉽도록 분리된 정보를 yield\n",
    "            yield {\n",
    "                'image_path': item['image'],\n",
    "                'question': question,\n",
    "                'answer': answer\n",
    "            }\n",
    "\n",
    "expanded_instruction_dataset = Dataset.from_generator(instruction_generator, gen_kwargs={\"dataset\": instruct_data})\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "\n",
    "    image_paths = examples['image_path'] \n",
    "    questions = examples['question']\n",
    "    answers = examples['answer']\n",
    "\n",
    "    # 이미지 처리 \n",
    "    images = [Image.open(f\"coco/train2017/{path}\").convert(\"RGB\") for path in image_paths]\n",
    "    processed_images = image_processor(images, return_tensors=\"pt\")\n",
    "\n",
    "    # 텍스트 처리 및 라벨 마스킹 \n",
    "    question_tokenized = tokenizer(questions, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    question_lengths = [len([tok for tok in ids if tok != tokenizer.pad_token_id]) for ids in question_tokenized['input_ids']]\n",
    "    \n",
    "    full_texts = [q + a for q, a in zip(questions, answers)]\n",
    "    model_inputs = tokenizer(full_texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # 위에서 계산한 질문 길이만큼 labels의 앞부분을 -100으로 마스킹\n",
    "    labels = model_inputs['input_ids'].clone()\n",
    "    for i, question_len in enumerate(question_lengths):\n",
    "        labels[i, :question_len] = -100\n",
    "\n",
    "    # 최종 결과물 조합\n",
    "    return {\n",
    "        \"pixel_values\": processed_images.pixel_values,\n",
    "        \"input_ids\": model_inputs.input_ids,\n",
    "        \"attention_mask\": model_inputs.attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "instruction_dataset = expanded_instruction_dataset.map(\n",
    "     function=preprocess_function,\n",
    "     batched=True,\n",
    "     remove_columns=expanded_instruction_dataset.column_names\n",
    ")\n",
    "\n",
    "save_path_instructions = \"./instruction_dataset\"\n",
    "print(f\"Saving the instruction dataset to '{save_path_instructions}'...\")\n",
    "\n",
    "instruction_dataset.save_to_disk(save_path_instructions)\n",
    "\n",
    "print(\"Instruction dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ca595",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "load_path = \"./instruction_dataset\"\n",
    "print(f\"Loading instruction dataset from '{load_path}'...\")\n",
    "\n",
    "instruction_dataset = load_from_disk(load_path)\n",
    "\n",
    "print(\"Instruction dataset reloaded successfully!\")\n",
    "print(instruction_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc705ea7",
   "metadata": {},
   "source": [
    "## 4. 최적의 파라미터 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8373685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW \n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import default_data_collator\n",
    "from tqdm.auto import tqdm\n",
    "import wandb \n",
    "import gc\n",
    "\n",
    "\n",
    "columns_to_tensorize = ['pixel_values', 'input_ids', 'attention_mask', 'labels']\n",
    "instruction_dataset.set_format(type='torch', columns=columns_to_tensorize)\n",
    "\n",
    "def model_train_eval(model, dataset, config, device):\n",
    "\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"instruction-tuning-test\",\n",
    "        name=f\"lr={config['learning_rate']}\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    test_final_dataset = dataset.select(range(10000)) #훈련 상황에 따라 바꾸면 됨\n",
    "    test_eval_final_dataset = dataset.select(range(10000, 11000)) # 테스트 데이터셋\n",
    "\n",
    "    # collate_fn = defalut_data_collator를 함으로써 데이터의 형태로 올바르게 맞춰 줌\n",
    "    train_dataloader = DataLoader(test_final_dataset, batch_size=config[\"batch_size\"], collate_fn=default_data_collator, shuffle=True)\n",
    "    eval_dataloader = DataLoader(test_eval_final_dataset, batch_size=config[\"batch_size\"], collate_fn=default_data_collator, shuffle=True)\n",
    "\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad] # require_grad가 허용된(미분가능) 부분에만 optimizer적용\n",
    "    optimizer = AdamW(trainable_params, lr=config[\"learning_rate\"])\n",
    " \n",
    "    num_training_steps = config[\"num_epochs\"] * len(train_dataloader)\n",
    "    num_warmup_steps = int(num_training_steps * 0.05)\n",
    "    \n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps= num_warmup_steps, # 몇 step동안 천천히 증가(웜업) 할지 설정\n",
    "        num_training_steps=num_training_steps, # 몇 step에 걸쳐 천천히 감소할지 설정\n",
    "    )\n",
    "    print(\"Optimizer and Scheduler have been set up.\")\n",
    "\n",
    "    # 학습 시작\n",
    "    print(f\"\\n--- Starting Training for {config['num_epochs']} epoch(s) ---\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 훈련 루프\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        print(train_dataloader)\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            outputs = model(batch['input_ids'].to(device),\n",
    "                                        batch['pixel_values'].to(device),\n",
    "                                        batch['attention_mask'].to(device),\n",
    "                                        batch['labels'].to(device))\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                    wandb.log({\"train/loss\": loss.item()})\n",
    "            \n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        # 검증 루프 \n",
    "        model.eval()\n",
    "        eval_loss_total = 0\n",
    "        print(f\"\\n--- Validating Epoch {epoch + 1} ---\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_dataloader, desc=\"Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                eval_loss_total += loss.item()\n",
    "        \n",
    "        avg_eval_loss = eval_loss_total / len(eval_dataloader)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"eval/loss\": avg_eval_loss,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "        \n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 텐서로 바꿔주기\n",
    "    columns_to_tensorize = ['pixel_values', 'input_ids', 'attention_mask', 'labels']\n",
    "    instruction_dataset.set_format(type='torch', columns=columns_to_tensorize)\n",
    "\n",
    "    parameters = {\n",
    "        \"learning_rate\": [1e-4, 5e-5, 3e-5, 1e-5, 5e-6, 1e-6, 5e-7]\n",
    "    }\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 파라미터별 데이터 성능 테스트\n",
    "    for i in range(1):\n",
    "            # 모델 로드\n",
    "        model, tokenizer, image_processor = load_my_trained_model(\"./save_model/3epoch\")\n",
    "        \n",
    "        config = {\"num_epochs\": 1,\n",
    "                        \"batch_size\": 7,\n",
    "                        \"learning_rate\": parameters['learning_rate'][i]}\n",
    "        \n",
    "        model_train_eval(model, instruction_dataset, config, device)\n",
    "\n",
    "        del model, image_processor, tokenizer\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "print(\"Model test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e5169",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, CLIPVisionModel, CLIPImageProcessor\n",
    "from datasets import load_from_disk\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, model_dims: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=model_dims, \n",
    "            num_heads=num_heads, \n",
    "            batch_first=True,\n",
    "            dtype=torch.bfloat16 # float16\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(model_dims)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        attn_output, _ = self.attention(text_features, image_features, image_features)\n",
    "        output = self.layer_norm(text_features + attn_output)\n",
    "        return output\n",
    "\n",
    "class MultimodalPhi2(nn.Module):\n",
    "    def __init__(self, peft_llm, vision_encoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.llm = peft_llm\n",
    "\n",
    "        target_dtype = self.llm.dtype\n",
    "        model_dims = self.llm.config.hidden_size\n",
    "        vit_dims = self.vision_encoder.config.hidden_size\n",
    "        num_heads = self.llm.config.num_attention_heads\n",
    "        num_llm_layers = self.llm.config.num_hidden_layers\n",
    "\n",
    "        self.target_layers = range(num_llm_layers - 4, num_llm_layers)\n",
    "\n",
    "        self.vision_projection = nn.Linear(vit_dims, model_dims)\n",
    "        self.vision_projection.to(device=self.llm.device, dtype=target_dtype) \n",
    "        \n",
    "        self.cross_attentions = nn.ModuleDict({\n",
    "        str(i): CrossAttention(model_dims, num_heads) for i in self.target_layers\n",
    "        }) \n",
    "\n",
    "        self.image_features_cache = None # 이미지 특징을 임시 저장할 공간\n",
    "\n",
    "        for layer_idx in self.target_layers:\n",
    "        # ModuleDict의 키는 문자열이므로, 인덱싱할 때 str(layer_idx)를 사용\n",
    "            layer = self.llm.model.model.layers[layer_idx] \n",
    "            layer.self_attn.register_forward_hook(\n",
    "                partial(self.cross_attention_hook, layer_idx=layer_idx)\n",
    "            )\n",
    "        \n",
    "        self.cross_attentions.to(device=self.llm.device, dtype=target_dtype)\n",
    "\n",
    "    def cross_attention_hook(self, module, input, output, layer_idx):\n",
    "\n",
    "        hidden_states = output[0]\n",
    "        \n",
    "        # ModuleDict의 키는 문자열이므로, 인덱싱할 때 str(layer_idx)를 사용\n",
    "        cross_attn_output = self.cross_attentions[str(layer_idx)](\n",
    "            hidden_states, self.image_features_cache\n",
    "        )\n",
    "\n",
    "        return (cross_attn_output,) + output[1:]\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, pixel_values: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        image_outputs = self.vision_encoder(pixel_values)\n",
    "        image_patch_features = image_outputs.last_hidden_state\n",
    "        self.image_features_cache = self.vision_projection(\n",
    "            image_patch_features.to(self.llm.dtype)\n",
    "        )\n",
    "\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels \n",
    "        )\n",
    "\n",
    "        self.image_features_cache = None\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "def load_my_trained_model(save_directory):\n",
    "    print(f\"Loading our trained model from {save_directory}...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", load_in_4bit=True, torch_dtype=torch.bfloat16).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "    vision_encoder = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\", torch_dtype=torch.bfloat16).to(device)\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\"\n",
    "    )\n",
    "\n",
    "    lora_save_path = os.path.join(save_directory, \"llm_adapters3\")\n",
    "    peft_llm = PeftModel.from_pretrained(llm, lora_save_path)\n",
    "\n",
    "    model = MultimodalPhi2(peft_llm, vision_encoder)\n",
    "\n",
    "    vision_projection_path = os.path.join(save_directory, \"vision_projection3.pt\")\n",
    "    model.vision_projection.load_state_dict(torch.load(vision_projection_path, map_location='cpu'))\n",
    "\n",
    "    cross_attentions_path = os.path.join(save_directory, \"cross_attentions3.pt\")\n",
    "    model.cross_attentions.load_state_dict(torch.load(cross_attentions_path, map_location='cpu'))\n",
    "    \n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"\\nOur final trained model has been successfully loaded and is ready for inference!\")\n",
    "    return model, tokenizer, image_processor\n",
    "\n",
    "model, tokenizer, image_processor = load_my_trained_model(\"./save_model/3epoch\")\n",
    "\n",
    "\n",
    "load_path = \"./instruction_dataset\"\n",
    "print(f\"Loading instruction dataset from '{load_path}'...\")\n",
    "\n",
    "instruction_dataset = load_from_disk(load_path)\n",
    "\n",
    "print(\"Instruction dataset reloaded successfully!\")\n",
    "print(instruction_dataset)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW \n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import default_data_collator\n",
    "from tqdm.auto import tqdm\n",
    "import wandb \n",
    "import os\n",
    "\n",
    "def model_train_eval(model, dataset, config, device):\n",
    "\n",
    "    save_directory = \"./save_instruction_train\"\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"instruction-tuning\",\n",
    "        name=f\"lr={config['learning_rate']} instruction-tuning\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    split_dataset = dataset.train_test_split(test_size=10000, shuffle=True, seed=42)\n",
    "\n",
    "    test_final_dataset = split_dataset['train']\n",
    "    test_eval_final_dataset = split_dataset['test']\n",
    "\n",
    "    train_dataloader = DataLoader(test_final_dataset, batch_size=config[\"batch_size\"], collate_fn=default_data_collator, shuffle=True)\n",
    "    eval_dataloader = DataLoader(test_eval_final_dataset, batch_size=config[\"batch_size\"], collate_fn=default_data_collator, shuffle=True)\n",
    "\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad] # require_grad가 허용된(미분가능) 부분에만 optimizer적용\n",
    "    optimizer = AdamW(trainable_params, lr=config[\"learning_rate\"])\n",
    " \n",
    "    num_training_steps = config[\"num_epochs\"] * len(train_dataloader)\n",
    "    num_warmup_steps = int(num_training_steps * 0.05)\n",
    "    \n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps= num_warmup_steps, # 몇 step동안 천천히 증가(웜업) 할지 설정\n",
    "        num_training_steps=num_training_steps, # 몇 step에 걸쳐 천천히 감소할지 설정\n",
    "    )\n",
    "    print(\"Optimizer and Scheduler have been set up.\")\n",
    "\n",
    "    # 학습 시작\n",
    "    print(f\"\\n--- Starting Training for {config['num_epochs']} epoch(s) ---\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 훈련 루프\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        print(train_dataloader)\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            outputs = model(batch['input_ids'].to(device),\n",
    "                                        batch['pixel_values'].to(device),\n",
    "                                        batch['attention_mask'].to(device),\n",
    "                                        batch['labels'].to(device))\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                    wandb.log({\"train/loss\": loss.item()})\n",
    "            \n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        # 검증 루프 \n",
    "        model.eval()\n",
    "        eval_loss_total = 0\n",
    "        print(f\"\\n--- Validating Epoch {epoch + 1} ---\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_dataloader, desc=\"Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                eval_loss_total += loss.item()\n",
    "        \n",
    "        avg_eval_loss = eval_loss_total / len(eval_dataloader)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"eval/loss\": avg_eval_loss,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        os.makedirs(save_directory+f\"/{epoch}epoch\", exist_ok=True)\n",
    "    \n",
    "        lora_save_path = os.path.join(save_directory+f\"/{epoch}epoch\", f\"llm_adapters{epoch}\")\n",
    "        model.llm.save_pretrained(lora_save_path)\n",
    "        print(f\"LoRA adapters saved to {lora_save_path}\")\n",
    "\n",
    "        vision_projection_path = os.path.join(save_directory+f\"/{epoch}epoch\", f\"vision_projection{epoch}.pt\")\n",
    "        torch.save(model.vision_projection.state_dict(), vision_projection_path)\n",
    "        print(f\"Vision projection saved to {vision_projection_path}\")\n",
    "\n",
    "        cross_attentions_path = os.path.join(save_directory+f\"/{epoch}epoch\", f\"cross_attentions{epoch}.pt\")\n",
    "        torch.save(model.cross_attentions.state_dict(), cross_attentions_path)\n",
    "        print(f\"Cross attentions saved to {cross_attentions_path}\")\n",
    "        \n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 텐서로 바꿔주기\n",
    "    columns_to_tensorize = ['pixel_values', 'input_ids', 'attention_mask', 'labels']\n",
    "    instruction_dataset.set_format(type='torch', columns=columns_to_tensorize)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    config = {\"num_epochs\": 5,\n",
    "                    \"batch_size\": 7,\n",
    "                    \"learning_rate\":1e-4} \n",
    "    \n",
    "    model_train_eval(model, instruction_dataset, config, device)\n",
    "\n",
    "print(\"Model test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b310c7",
   "metadata": {},
   "source": [
    "## 5. 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ff03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_dataset = pd.read_csv(\"./open/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2284f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추론\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.functional import F\n",
    "\n",
    "def inference(model, tokenizer, image_processor, prompt, image_path):\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                print(\"Tokenizer pad_token is set to eos_token.\")\n",
    "\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = prompt_tokens.input_ids.to(device)\n",
    "    attention_mask = prompt_tokens.attention_mask.to(device)\n",
    "    PIL_image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image_processor(images=PIL_image, return_tensors=\"pt\").pixel_values.to(device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_outputs = model.vision_encoder(image)\n",
    "        image_patch_features = image_outputs.last_hidden_state\n",
    "        model.image_features_cache = model.vision_projection(image_patch_features)\n",
    "    \n",
    "    generated_ids = model.llm.generate(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        max_new_tokens=128,  # 새로 생성할 최대 토큰 수\n",
    "        do_sample=False,     # 샘플링을 활성화\n",
    "        temperature=1,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    model.image_features_cache = None\n",
    "\n",
    "    input_token_len = input_ids.shape[1]\n",
    "    generated_text_ids = generated_ids[:, input_token_len:]\n",
    "    \n",
    "    generated_text = tokenizer.batch_decode(generated_text_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text.strip()\n",
    "\n",
    "# 기존 질문 내용\n",
    "\n",
    "index = 1\n",
    "\n",
    "image_path = \"./open/\"+test_dataset['img_path'][index]\n",
    "\n",
    "question = test_dataset['Question'][index]+\" A.\"+ test_dataset['A'][index] + \" B.\" + test_dataset['B'][index] + \" C.\" + test_dataset['C'][index] + \" D.\" + test_dataset['D'][index]\n",
    "prompt = f\"\"\"ROLE: You're an ASSISTANT, I give you an image and a question, and you answer it for me.\\n \n",
    "            RULE: Please select only one correct answer from A,B,C,D\\n\n",
    "            USER: {question}\\n\n",
    "            ASSISTANT:\"\"\"\n",
    "\n",
    "result = inference(model, tokenizer, image_processor, prompt, image_path)\n",
    "\n",
    "plt.imshow(plt.imread(image_path))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Generated text: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
